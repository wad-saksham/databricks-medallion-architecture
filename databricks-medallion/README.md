# ğŸ… Databricks Medallion Architecture Project

> Production-grade data lakehouse implementation using Databricks' Medallion Architecture pattern with Bronze-Silver-Gold layers.

![Project Status](https://img.shields.io/badge/status-active-success.svg)
![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![PySpark](https://img.shields.io/badge/PySpark-3.5.0-orange.svg)
![Delta Lake](https://img.shields.io/badge/Delta_Lake-3.0.0-red.svg)

---

## ğŸ“– Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Technologies](#technologies)
- [Project Structure](#project-structure)
- [Setup Instructions](#setup-instructions)
- [Usage](#usage)
- [Key Features](#key-features)
- [Learning Outcomes](#learning-outcomes)
- [Future Enhancements](#future-enhancements)

---

## ğŸ¯ Overview

This project demonstrates a **production-ready data lakehouse** implementation using the Medallion Architecture pattern. The pipeline processes IoT sensor data through three progressive layers:

- **Bronze Layer**: Raw data ingestion with auditability
- **Silver Layer**: Cleaned, validated, and conformed data
- **Gold Layer**: Business-ready aggregations for analytics

### Business Use Case

Monitoring and analyzing IoT sensor data from manufacturing facilities to detect anomalies, track performance metrics, and optimize operations.

---

## ğŸ—ï¸ Architecture

### Medallion Architecture Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES                         â”‚
â”‚                    (IoT Sensors, APIs, Files)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BRONZE LAYER (Raw)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Exact copy of source data                          â”‚  â”‚
â”‚  â”‚ â€¢ Append-only Delta tables                           â”‚  â”‚
â”‚  â”‚ â€¢ Includes ingestion metadata                        â”‚  â”‚
â”‚  â”‚ â€¢ Schema validation on write                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SILVER LAYER (Refined)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Data cleansing (nulls, duplicates)                 â”‚  â”‚
â”‚  â”‚ â€¢ Type casting and format standardization            â”‚  â”‚
â”‚  â”‚ â€¢ Business rule validation                           â”‚  â”‚
â”‚  â”‚ â€¢ Data quality checks                                â”‚  â”‚
â”‚  â”‚ â€¢ Incremental merge using Delta                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GOLD LAYER (Curated)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Pre-aggregated metrics                             â”‚  â”‚
â”‚  â”‚ â€¢ Denormalized for query performance                 â”‚  â”‚
â”‚  â”‚ â€¢ Business-level transformations                     â”‚  â”‚
â”‚  â”‚ â€¢ Ready for BI tools and ML                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ANALYTICS & BI LAYER                      â”‚
â”‚              (Dashboards, Reports, ML Models)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Ingestion**: IoT sensor data arrives as JSON files in S3
2. **Bronze**: Raw data loaded into Delta Lake with metadata
3. **Silver**: Data cleaned, deduplicated, and validated
4. **Gold**: Aggregated metrics for analytics use cases
5. **Consumption**: Business users query Gold tables for insights

---

## ğŸ› ï¸ Technologies

| Technology             | Version   | Purpose                        |
| ---------------------- | --------- | ------------------------------ |
| **Python**             | 3.9+      | Primary programming language   |
| **PySpark**            | 3.5.0     | Distributed data processing    |
| **Delta Lake**         | 3.0.0     | ACID transactions, time travel |
| **Databricks**         | Community | Unified analytics platform     |
| **AWS S3**             | -         | Cloud object storage           |
| **Boto3**              | 1.28+     | AWS SDK for Python             |
| **Great Expectations** | 0.18+     | Data quality validation        |

---

## ğŸ“ Project Structure

```
databricks-medallion/
â”‚
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ notebooks/                         # Databricks notebooks
â”‚   â”œâ”€â”€ 00_setup_environment.py       # Initial setup and config
â”‚   â”œâ”€â”€ 01_bronze_layer.py            # Raw data ingestion
â”‚   â”œâ”€â”€ 02_silver_layer.py            # Data cleansing & validation
â”‚   â”œâ”€â”€ 03_gold_layer.py              # Business aggregations
â”‚   â””â”€â”€ 04_data_quality_checks.py     # Quality validation
â”‚
â”œâ”€â”€ src/                               # Reusable Python modules
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ config.py                 # Configuration management
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ spark_utils.py            # Spark helper functions
â”‚   â”‚   â”œâ”€â”€ delta_utils.py            # Delta Lake operations
â”‚   â”‚   â””â”€â”€ data_generator.py         # Sample data generation
â”‚   â”‚
â”‚   â””â”€â”€ quality/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ validators.py             # Data quality checks
â”‚       â””â”€â”€ expectations.py           # Great Expectations suite
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample/                        # Sample data for testing
â”‚       â”œâ”€â”€ iot_sensors_sample.json
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ tests/                             # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â””â”€â”€ test_quality_checks.py
â”‚
â””â”€â”€ docs/                              # Documentation
    â”œâ”€â”€ architecture_diagram.md
    â”œâ”€â”€ data_dictionary.md
    â””â”€â”€ deployment_guide.md
```

---

## âš™ï¸ Setup Instructions

### Prerequisites

1. **Databricks Community Edition Account**
   - Sign up at: https://community.cloud.databricks.com/

2. **AWS Free Tier Account**
   - Sign up at: https://aws.amazon.com/free/

3. **Local Development Tools**

   ```powershell
   # Python 3.9+
   python --version

   # Git
   git --version
   ```

### Installation Steps

#### 1. Clone Repository

```powershell
cd "c:\Users\saksh\OneDrive\Desktop\Data enginnering project"
```

#### 2. Create Virtual Environment

```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

#### 3. Install Dependencies

```powershell
pip install -r ../requirements.txt
```

#### 4. Configure AWS Credentials

```powershell
aws configure
# Enter your AWS Access Key, Secret Key, and region (us-east-1)
```

#### 5. Create S3 Bucket

```powershell
aws s3 mb s3://your-name-medallion-demo
```

#### 6. Update Configuration

```python
# Edit src/config/config.py
BUCKET_NAME = "your-name-medallion-demo"
AWS_REGION = "us-east-1"
```

---

## ğŸš€ Usage

### Option 1: Run Locally (Testing)

```powershell
# Activate virtual environment
.\venv\Scripts\Activate.ps1

# Navigate to project
cd databricks-medallion

# Generate sample data
python src/utils/data_generator.py

# Run Bronze layer (local mode)
python notebooks/01_bronze_layer.py

# Run Silver layer
python notebooks/02_silver_layer.py

# Run Gold layer
python notebooks/03_gold_layer.py

# Run quality checks
python notebooks/04_data_quality_checks.py
```

### Option 2: Run on Databricks (Production)

1. **Upload Notebooks**
   - Login to Databricks workspace
   - Navigate to Workspace â†’ Import
   - Upload all files from `notebooks/` folder

2. **Upload Source Code**
   - Create folder: `/Workspace/Users/your-email/medallion-project/`
   - Upload `src/` directory

3. **Create Cluster**
   - Runtime: 13.3 LTS (Scala 2.12, Spark 3.4.1)
   - Node Type: Single node (for Community Edition)

4. **Run Notebooks in Order**
   - Execute: `00_setup_environment`
   - Execute: `01_bronze_layer`
   - Execute: `02_silver_layer`
   - Execute: `03_gold_layer`
   - Execute: `04_data_quality_checks`

5. **Schedule Jobs** (if available)
   - Go to Workflows â†’ Create Job
   - Add notebook tasks
   - Set schedule (daily/hourly)

---

## âœ¨ Key Features

### 1. **ACID Transactions with Delta Lake**

```python
# Atomic writes ensure data consistency
df.write
    .format("delta")
    .mode("append")
    .save(bronze_path)
```

### 2. **Incremental Processing**

```python
# Process only new data since last run
deltaTable.alias("target").merge(
    source.alias("source"),
    "target.sensor_id = source.sensor_id AND target.timestamp = source.timestamp"
).whenNotMatchedInsertAll().execute()
```

### 3. **Time Travel**

```python
# Query historical versions
df_yesterday = spark.read
    .format("delta")
    .option("versionAsOf", 10)
    .load(silver_path)
```

### 4. **Data Quality Validation**

```python
# Automated quality checks
quality_checks = [
    ("null_rate_check", null_percentage < 5%),
    ("duplicate_check", duplicate_count == 0),
    ("schema_validation", schema_matches_expected)
]
```

### 5. **Schema Evolution**

```python
# Handle schema changes gracefully
df.write
    .format("delta")
    .option("mergeSchema", "true")
    .mode("append")
    .save(path)
```

### 6. **Partitioning for Performance**

```python
# Optimize queries with partitioning
df.write
    .format("delta")
    .partitionBy("date", "sensor_type")
    .save(gold_path)
```

---

## ğŸ“ Learning Outcomes

After completing this project, you will understand:

### Technical Skills

- âœ… Medallion Architecture design pattern
- âœ… Delta Lake ACID transactions
- âœ… PySpark DataFrame transformations
- âœ… Incremental data processing
- âœ… Data quality frameworks
- âœ… Cloud storage integration (S3)
- âœ… Databricks platform usage

### Best Practices

- âœ… Separation of concerns (Bronze/Silver/Gold)
- âœ… Schema validation and evolution
- âœ… Error handling and logging
- âœ… Configuration management
- âœ… Code modularity and reusability
- âœ… Testing data pipelines

### Interview Topics

- âœ… "Explain Medallion Architecture"
- âœ… "Why use Delta Lake over Parquet?"
- âœ… "How do you handle late-arriving data?"
- âœ… "Describe your data quality approach"
- âœ… "How do you optimize Spark performance?"

---

## ğŸ”® Future Enhancements

### Phase 2 (Streaming)

- [ ] Implement streaming ingestion with Structured Streaming
- [ ] Add watermarking for late data handling
- [ ] Real-time dashboard with live metrics

### Phase 3 (Orchestration)

- [ ] Apache Airflow DAGs for scheduling
- [ ] Monitoring and alerting with Databricks
- [ ] CI/CD pipeline with GitHub Actions

### Phase 4 (Advanced)

- [ ] Unity Catalog for data governance
- [ ] ML models on Gold layer data
- [ ] Advanced partitioning strategies
- [ ] Query optimization techniques

---

## ğŸ“Š Sample Queries

### Query Gold Layer Metrics

```sql
-- Top performing sensors by average reading
SELECT
    sensor_id,
    sensor_type,
    AVG(reading_value) as avg_reading,
    COUNT(*) as total_readings,
    MAX(reading_value) as max_reading
FROM gold.sensor_metrics_daily
WHERE date >= CURRENT_DATE - INTERVAL 7 DAYS
GROUP BY sensor_id, sensor_type
ORDER BY avg_reading DESC
LIMIT 10;
```

### Data Quality Report

```sql
-- Quality metrics by layer
SELECT
    layer,
    check_name,
    passed,
    check_timestamp
FROM quality.validation_results
WHERE DATE(check_timestamp) = CURRENT_DATE
ORDER BY check_timestamp DESC;
```

---

## ğŸ¤ Contributing

This is a portfolio project, but feedback and suggestions are welcome!

---

## ğŸ“œ License

MIT License - Feel free to use this for learning purposes.

---

## ğŸ‘¤ Author

**Your Name**

- LinkedIn: [your-linkedin]
- GitHub: [your-github]
- Email: [your-email]

---

## ğŸ™ Acknowledgments

- Databricks documentation and community
- Apache Spark community
- Delta Lake contributors

---

**Built with â¤ï¸ to demonstrate modern data engineering skills**
